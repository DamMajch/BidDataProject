from delta import configure_spark_with_delta_pip
from pyspark.sql import SparkSession
import os
import sys

# Debugowanie ≈õcie≈ºek modu≈Ç√≥w
print("üîç ≈öcie≈ºki modu≈Ç√≥w Pythona:", sys.path)

# Sprawd≈∫ import modu≈Çu typing
try:
    import typing

    print("‚úÖ Modu≈Ç typing zaimportowany pomy≈õlnie z:", typing.__file__)
except Exception as e:
    print("‚ùå B≈ÇƒÖd importu modu≈Çu typing:", e)
    raise ImportError("Modu≈Ç typing jest uszkodzony lub konfliktuje z lokalnym plikiem/folderem.")

# Konfiguracja zmiennych ≈õrodowiskowych
os.environ["JAVA_HOME"] = "C:\\Program Files\\Java\\jdk-11.0.25.9-hotspot"
os.environ["SPARK_HOME"] = "C:\\spark-3.5.5-bin-hadoop3"

# Debugowanie zmiennych ≈õrodowiskowych
print(f"üîç JAVA_HOME ustawiony na: {os.environ['JAVA_HOME']}")
print(f"üîç SPARK_HOME ustawiony na: {os.environ['SPARK_HOME']}")

# Weryfikacja zmiennych ≈õrodowiskowych
if not os.path.exists(os.environ["JAVA_HOME"]):
    raise FileNotFoundError(f"B≈ÇƒÖd: ≈öcie≈ºka JAVA_HOME ({os.environ['JAVA_HOME']}) nie istnieje.")
if not os.path.exists(os.environ["SPARK_HOME"]):
    raise FileNotFoundError(f"B≈ÇƒÖd: ≈öcie≈ºka SPARK_HOME ({os.environ['SPARK_HOME']}) nie istnieje.")

# Inicjalizacja SparkSession
builder = SparkSession.builder \
    .appName("DeltaSave") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")

try:
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    print("‚úÖ SparkSession zosta≈Ç zainicjalizowany pomy≈õlnie.")
except Exception as e:
    print("‚ùå B≈ÇƒÖd inicjalizacji SparkSession.")
    raise RuntimeError(
        "B≈ÇƒÖd inicjalizacji SparkSession. "
        "Upewnij siƒô, ≈ºe JAVA_HOME i SPARK_HOME sƒÖ poprawnie skonfigurowane oraz ≈ºe Spark dzia≈Ça.") from e

# Za≈Çaduj dane z CSV
csv_path = "data/processed/alcohol_data.csv"
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"Plik CSV ({csv_path}) nie zosta≈Ç znaleziony.")

df = spark.read.option("header", True).csv(csv_path)

# Zapis jako Delta Lake
delta_path = "data/delta/alcohol"
# noinspection PyPackageRequirements
try:
    df.write.format("delta").mode("overwrite").save(delta_path)
    print(f"‚úÖ Dane zapisane jako Delta Lake w: {delta_path}")
except Exception as e:
    raise RuntimeError(f"Nie uda≈Ço siƒô zapisaƒá danych w Delta Lake pod ≈õcie≈ºkƒÖ: {delta_path}.") from e
